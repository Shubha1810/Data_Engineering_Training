{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf5dd803-cd2c-4b16-b425-ca5b756c6e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will store data in:\nLanding Orders folder:    dbfs:/FileStore/tables/dlt/landing/orders\nLanding Customers folder: dbfs:/FileStore/tables/dlt/landing/customers\nSilver Delta folder:      dbfs:/tmp/delta/sil_orders\nSQL Table name:           sil_orders_tbl\n"
     ]
    }
   ],
   "source": [
    "# STEP 0 — Setup\n",
    "# These variables store the paths for each stage of the pipeline\n",
    "\n",
    "# Landing folders: raw files exactly as they arrive\n",
    "LANDING_ORDERS    = \"dbfs:/FileStore/tables/dlt/landing/orders\"\n",
    "LANDING_CUSTOMERS = \"dbfs:/FileStore/tables/dlt/landing/customers\"\n",
    "\n",
    "# Silver folder: cleaned data in Delta format\n",
    "DELTA_SILVER_PATH = \"dbfs:/tmp/delta/sil_orders\" # silver means cleaned data\n",
    "\n",
    "# SQL table name pointing to the silver Delta folder\n",
    "DELTA_TABLE_NAME  = \"sil_orders_tbl\"\n",
    "\n",
    "print(\"We will store data in:\")\n",
    "print(f\"Landing Orders folder:    {LANDING_ORDERS}\")\n",
    "print(f\"Landing Customers folder: {LANDING_CUSTOMERS}\")\n",
    "print(f\"Silver Delta folder:      {DELTA_SILVER_PATH}\")\n",
    "print(f\"SQL Table name:           {DELTA_TABLE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6387a6dd-bf8c-4ca9-bb6e-ae1c81f73d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de21f6c-4473-4671-a9fc-34e7d9ddd6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Seeding inline data to landing (JSON) ...\n✅ Seeded landing JSON:\n  dbfs:/FileStore/tables/dlt/landing/orders\n  dbfs:/FileStore/tables/dlt/landing/customers\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 1: Seeding inline data to landing (JSON) ...\")\n",
    "\n",
    "orders_rows = [\n",
    "    (1, \"C001\", \"2025-08-08 09:00:00\", 12000, \"placed\"),\n",
    "    (2, \"C002\", \"2025-08-08 09:05:00\",  4500, \"placed\"),\n",
    "    (3, \"C001\", \"2025-08-08 09:10:00\", 22000, \"cancelled\"),\n",
    "    (4, \"C003\", \"2025-08-08 09:15:00\",   800, \"placed\")\n",
    "]\n",
    "customers_rows = [\n",
    "    (\"C001\", \"Ananya\", \"Bengaluru\"),\n",
    "    (\"C002\", \"Rahul\",  \"Hyderabad\"),\n",
    "    (\"C003\", \"Meera\",  \"Pune\")\n",
    "]\n",
    "\n",
    "orders_schema = T.StructType([\n",
    "    T.StructField(\"order_id\",    T.IntegerType()),\n",
    "    T.StructField(\"customer_id\", T.StringType()),\n",
    "    T.StructField(\"order_ts\",    T.StringType()),\n",
    "    T.StructField(\"amount\",      T.IntegerType()),\n",
    "    T.StructField(\"status\",      T.StringType())\n",
    "])\n",
    "cust_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.StringType()),\n",
    "    T.StructField(\"name\",        T.StringType()),\n",
    "    T.StructField(\"city\",        T.StringType())\n",
    "])\n",
    "\n",
    "orders_df = (spark.createDataFrame(orders_rows, orders_schema)\n",
    "             .withColumn(\"order_ts\", F.to_timestamp(\"order_ts\")))\n",
    "customers_df = spark.createDataFrame(customers_rows, cust_schema)\n",
    "\n",
    "orders_df.write.mode(\"overwrite\").json(LANDING_ORDERS)\n",
    "customers_df.write.mode(\"overwrite\").json(LANDING_CUSTOMERS)\n",
    "\n",
    "print(\"✅ Seeded landing JSON:\")\n",
    "print(f\"  {LANDING_ORDERS}\")\n",
    "print(f\"  {LANDING_CUSTOMERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834df314-269f-4008-aff2-ebdb91a5058d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: BRONZE Reading raw landing data (no transformations)\nBronze Orders - schema & sample:\nroot\n |-- amount: long (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_id: long (nullable = true)\n |-- order_ts: string (nullable = true)\n |-- status: string (nullable = true)\n\n+------+-----------+--------+------------------------+---------+\n|amount|customer_id|order_id|order_ts                |status   |\n+------+-----------+--------+------------------------+---------+\n|22000 |C001       |3       |2025-08-08T09:10:00.000Z|cancelled|\n|12000 |C001       |1       |2025-08-08T09:00:00.000Z|placed   |\n|4500  |C002       |2       |2025-08-08T09:05:00.000Z|placed   |\n|800   |C003       |4       |2025-08-08T09:15:00.000Z|placed   |\n+------+-----------+--------+------------------------+---------+\n\nBronze Customers -  schema & sample:\nroot\n |-- city: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- name: string (nullable = true)\n\n+---------+-----------+------+\n|city     |customer_id|name  |\n+---------+-----------+------+\n|Bengaluru|C001       |Ananya|\n|Hyderabad|C002       |Rahul |\n|Pune     |C003       |Meera |\n+---------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: BRONZE Reading raw landing data (no transformations)\") # raw  data\n",
    "\n",
    "bron_orders = spark.read.json (LANDING_ORDERS)\n",
    "bron_customers =  spark.read.json(LANDING_CUSTOMERS)\n",
    "\n",
    "print(\"Bronze Orders - schema & sample:\")\n",
    "bron_orders.printSchema()\n",
    "bron_orders.show(truncate=False)\n",
    "\n",
    "print(\"Bronze Customers -  schema & sample:\")\n",
    "bron_customers.printSchema ()\n",
    "bron_customers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a304f86-903c-4ad4-9e2d-1520fdd27a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: SILVER - Cleaning data & writing to Delta\nWrote silver orders to Delta path:\n dbfs:/tmp/delta/sil_orders\nReading back from Delta to verify:\n+--------+-----------+------------------------+------+---------+\n|order_id|customer_id|order_ts                |amount|status   |\n+--------+-----------+------------------------+------+---------+\n|3       |C001       |2025-08-08T09:10:00.000Z|22000 |cancelled|\n|1       |C001       |2025-08-08T09:00:00.000Z|12000 |placed   |\n|2       |C002       |2025-08-08T09:05:00.000Z|4500  |placed   |\n|4       |C003       |2025-08-08T09:15:00.000Z|800   |placed   |\n+--------+-----------+------------------------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 3: SILVER - Cleaning data & writing to Delta\")\n",
    "\n",
    "sil_orders = (\n",
    "    bron_orders\n",
    "    .select(\"order_id\", \"customer_id\", \"order_ts\",\"amount\", \"status\")\n",
    "    .filter(\"order_id IS NOT NULL AND amount >= 0\")\n",
    ")\n",
    "\n",
    "#Write to Delta (overwrite for a clean slate)\n",
    "sil_orders.write.format(\"delta\").mode(\"overwrite\").save(DELTA_SILVER_PATH)\n",
    "\n",
    "print(\"Wrote silver orders to Delta path:\")\n",
    "print(f\" {DELTA_SILVER_PATH}\")\n",
    "\n",
    "print(\"Reading back from Delta to verify:\")\n",
    "spark.read.format(\"delta\").load(DELTA_SILVER_PATH).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be01df39-9b5e-4bc2-9ddf-c5bd1611a5f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: GOLD Enrich orders by joining with customers\nGold Enriched - sample:\n+-----------+--------+------------------------+------+---------+---------+------+\n|customer_id|order_id|order_ts                |amount|status   |city     |name  |\n+-----------+--------+------------------------+------+---------+---------+------+\n|C001       |3       |2025-08-08T09:10:00.000Z|22000 |cancelled|Bengaluru|Ananya|\n|C001       |1       |2025-08-08T09:00:00.000Z|12000 |placed   |Bengaluru|Ananya|\n|C002       |2       |2025-08-08T09:05:00.000Z|4500  |placed   |Hyderabad|Rahul |\n|C003       |4       |2025-08-08T09:15:00.000Z|800   |placed   |Pune     |Meera |\n+-----------+--------+------------------------+------+---------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 5: GOLD Enrich orders by joining with customers\")\n",
    "\n",
    "sil_orders_df = spark.read.format(\"delta\").load(DELTA_SILVER_PATH)\n",
    "\n",
    "gold_enriched = (\n",
    "    sil_orders_df.alias(\"o\")\n",
    "    .join(bron_customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"Gold Enriched - sample:\")\n",
    "gold_enriched.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fad4a0-a0c6-4293-bb7b-1dc252498892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# STEP 3 — Write Bronze Orders to Delta (Silver Layer)\n",
    "bron_orders.write.format(\"delta\").mode(\"overwrite\").save(DELTA_SILVER_PATH)\n",
    "\n",
    "# Load Delta table from path\n",
    "delta_table = DeltaTable.forPath(spark, DELTA_SILVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1993e0-e9b3-482b-9f6a-94195b65b31a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After UPDATE (order_id=2 → shipped):\n+--------+-----------+------------------------+------+-------+\n|order_id|customer_id|order_ts                |amount|status |\n+--------+-----------+------------------------+------+-------+\n|1       |C001       |2025-08-08T09:00:00.000Z|12000 |placed |\n|4       |C003       |2025-08-08T09:15:00.000Z|800   |placed |\n|2       |C002       |2025-08-08 09:05:00     |5000  |shipped|\n|5       |C004       |2025-08-08 09:20:00     |3000  |placed |\n+--------+-----------+------------------------+------+-------+\n\nAfter DELETE (removed cancelled orders):\n+--------+-----------+------------------------+------+-------+\n|order_id|customer_id|order_ts                |amount|status |\n+--------+-----------+------------------------+------+-------+\n|1       |C001       |2025-08-08T09:00:00.000Z|12000 |placed |\n|4       |C003       |2025-08-08T09:15:00.000Z|800   |placed |\n|2       |C002       |2025-08-08 09:05:00     |5000  |shipped|\n|5       |C004       |2025-08-08 09:20:00     |3000  |placed |\n+--------+-----------+------------------------+------+-------+\n\nAfter UPSERT (order_id=2 updated, order_id=5 inserted):\n+--------+-----------+------------------------+------+---------+\n|order_id|customer_id|order_ts                |amount|status   |\n+--------+-----------+------------------------+------+---------+\n|1       |C001       |2025-08-08T09:00:00.000Z|12000 |placed   |\n|4       |C003       |2025-08-08T09:15:00.000Z|800   |placed   |\n|2       |C002       |2025-08-08 09:05:00     |5000  |delivered|\n|5       |C004       |2025-08-08 09:20:00     |3000  |placed   |\n+--------+-----------+------------------------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================== UPDATE =====================\n",
    "# Example: Change status of order_id=2 from \"placed\" to \"shipped\"\n",
    "delta_table.update(\n",
    "    condition=\"order_id = 2\",\n",
    "    set={\"status\": F.lit(\"shipped\")}\n",
    ")\n",
    "print(\"After UPDATE (order_id=2 → shipped):\")\n",
    "delta_table.toDF().show(truncate=False)\n",
    "\n",
    "# ===================== DELETE =====================\n",
    "# Example: Remove cancelled orders\n",
    "delta_table.delete(\n",
    "    condition=\"status = 'cancelled'\"\n",
    ")\n",
    "print(\"After DELETE (removed cancelled orders):\")\n",
    "delta_table.toDF().show(truncate=False)\n",
    "\n",
    "# ===================== UPSERT (MERGE) =====================\n",
    "# New incoming data with some updates and some new rows\n",
    "new_orders = [\n",
    "    (2, \"C002\", \"2025-08-08 09:05:00\",  5000, \"delivered\"), # updated\n",
    "    (5, \"C004\", \"2025-08-08 09:20:00\",  3000, \"placed\")     # new\n",
    "]\n",
    "\n",
    "orders_schema = T.StructType([\n",
    "    T.StructField(\"order_id\",    T.IntegerType()),\n",
    "    T.StructField(\"customer_id\", T.StringType()),\n",
    "    T.StructField(\"order_ts\",    T.StringType()),\n",
    "    T.StructField(\"amount\",      T.IntegerType()),\n",
    "    T.StructField(\"status\",      T.StringType())\n",
    "])\n",
    "\n",
    "new_orders_df = (\n",
    "    spark.createDataFrame(new_orders, orders_schema)\n",
    "         .withColumn(\"order_ts\", F.to_timestamp(\"order_ts\"))\n",
    ")\n",
    "\n",
    "# MERGE: Update matching rows and insert new ones\n",
    "(delta_table.alias(\"target\")\n",
    " .merge(\n",
    "     new_orders_df.alias(\"source\"),\n",
    "     \"target.order_id = source.order_id\"\n",
    " )\n",
    " .whenMatchedUpdateAll()\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute()\n",
    ")\n",
    "\n",
    "print(\"After UPSERT (order_id=2 updated, order_id=5 inserted):\")\n",
    "delta_table.toDF().show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_table_sir_notes2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}