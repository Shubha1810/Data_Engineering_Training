{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "xqBVui3mtPXW",
        "outputId": "4205fa31-7c68-4037-d19b-16e761e390f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c9bb6c413d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3b4e15883192:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkBasics</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Create Spark session\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"PySparkBasics\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "# Confirm Spark is running\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(\"Amit\", 25), (\"Sneha\", 30), (\"Kabir\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "#Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show data\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUn27uJPuPrm",
        "outputId": "17cd2062-97c7-40d8-c39c-3a579ba2f8fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "| Amit| 25|\n",
            "|Sneha| 30|\n",
            "|Kabir| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "csv_data = \"\"\"Name, Department, Salary\n",
        "Amit, Sales, 50000\n",
        "Sneha, Engineering, 80000\n",
        "Kabir, HR, 45000\n",
        "Anaya, Marketing, 60000\n",
        "Ravi, Engineering, 85000\"\"\"\n",
        "\n",
        "with open('employees.csv', 'w') as f:\n",
        "  f.write(csv_data)"
      ],
      "metadata": {
        "id": "iWiEqQ8O2YrZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('employees.csv', header = True, inferSchema=True)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuQpBIo-2eRy",
        "outputId": "e4098875-2383-4565-c333-9e77b4359767"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+-------+\n",
            "| Name|  Department| Salary|\n",
            "+-----+------------+-------+\n",
            "| Amit|       Sales|50000.0|\n",
            "|Sneha| Engineering|80000.0|\n",
            "|Kabir|          HR|45000.0|\n",
            "|Anaya|   Marketing|60000.0|\n",
            "| Ravi| Engineering|85000.0|\n",
            "+-----+------------+-------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |--  Department: string (nullable = true)\n",
            " |--  Salary: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT"
      ],
      "metadata": {
        "id": "_CWRDKX-7-ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Environment Setup\n",
        "\n",
        "1. Install Spark + Java in Google Colab.\n",
        "2. Initialize Spark with app name \"ProductSalesAnalysis\" ."
      ],
      "metadata": {
        "id": "wxQLBBxr8QeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java and Spark\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxby_BfY8FxQ",
        "outputId": "b12fbefa-d243-4936-e0f2-a41593eb3518"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk\n",
            "  openjdk-11-jre x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 5,367 kB of archives.\n",
            "After this operation, 15.2 MB of additional disk space will be used.\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Fetched 5,367 kB in 1s (6,374 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../2-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../3-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../5-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../6-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../7-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "tar: spark-3.4.1-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ProductSalesAnalysis\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "Oem1YXh28jGP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Load Sales Data from CSV\n",
        "\n",
        "Read the file into a PySpark DataFrame with header and inferred schema.\n",
        "Print schema and show top 5 rows.\n"
      ],
      "metadata": {
        "id": "GA5HglU58UgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = \"\"\"OrderID,Product,Category,Quantity,UnitPrice,Region\n",
        "1001,Mobile,Electronics,2,15000,North\n",
        "1002,Laptop,Electronics,1,55000,South\n",
        "1003,T-Shirt,Apparel,3,500,East\n",
        "1004,Jeans,Apparel,2,1200,North\n",
        "1005,TV,Electronics,1,40000,West\n",
        "1006,Shoes,Footwear,4,2000,South\n",
        "1007,Watch,Accessories,2,3000,East\n",
        "1008,Headphones,Electronics,3,2500,North\n",
        "\"\"\"\n",
        "\n",
        "with open(\"sales.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n"
      ],
      "metadata": {
        "id": "ZsmiVSNM8LKf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9XKl9a08rLJ",
        "outputId": "ed00c2a5-8d7d-48ca-c036-0e3189658577"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- UnitPrice: integer (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            "\n",
            "+-------+-------+-----------+--------+---------+------+\n",
            "|OrderID|Product|   Category|Quantity|UnitPrice|Region|\n",
            "+-------+-------+-----------+--------+---------+------+\n",
            "|   1001| Mobile|Electronics|       2|    15000| North|\n",
            "|   1002| Laptop|Electronics|       1|    55000| South|\n",
            "|   1003|T-Shirt|    Apparel|       3|      500|  East|\n",
            "|   1004|  Jeans|    Apparel|       2|     1200| North|\n",
            "|   1005|     TV|Electronics|       1|    40000|  West|\n",
            "+-------+-------+-----------+--------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Business Questions\n",
        "\n",
        "1. Add a new column TotalPrice = Quantity × UnitPrice\n",
        "2. Total revenue generated across all regions.\n",
        "3. Category-wise revenue sorted in descending order.\n",
        "4. Region with the highest number of orders\n",
        "5. Average Unit Price per Category\n",
        "6. All orders where TotalPrice is more than\n",
        "30,000"
      ],
      "metadata": {
        "id": "SdUI26l-8wXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W2FkG3B8tNN",
        "outputId": "f1cff9ff-fffd-4711-fcbf-c10cac0635d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+--------+---------+------+----------+\n",
            "|OrderID|   Product|   Category|Quantity|UnitPrice|Region|TotalPrice|\n",
            "+-------+----------+-----------+--------+---------+------+----------+\n",
            "|   1001|    Mobile|Electronics|       2|    15000| North|     30000|\n",
            "|   1002|    Laptop|Electronics|       1|    55000| South|     55000|\n",
            "|   1003|   T-Shirt|    Apparel|       3|      500|  East|      1500|\n",
            "|   1004|     Jeans|    Apparel|       2|     1200| North|      2400|\n",
            "|   1005|        TV|Electronics|       1|    40000|  West|     40000|\n",
            "|   1006|     Shoes|   Footwear|       4|     2000| South|      8000|\n",
            "|   1007|     Watch|Accessories|       2|     3000|  East|      6000|\n",
            "|   1008|Headphones|Electronics|       3|     2500| North|      7500|\n",
            "+-------+----------+-----------+--------+---------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_revenue = df.agg({\"TotalPrice\": \"sum\"}).collect()[0][0]\n",
        "print(f\"Total Revenue: ₹{total_revenue}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYWWBJq482pH",
        "outputId": "aa397e0f-d4ee-4f99-cbec-ce939197cae1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue: ₹150400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Category\") \\\n",
        "  .sum(\"TotalPrice\") \\\n",
        "  .withColumnRenamed(\"sum(TotalPrice)\", \"TotalRevenue\") \\\n",
        "  .orderBy(col(\"TotalRevenue\").desc()) \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtBOYgX-85sg",
        "outputId": "ae827295-0748-4f12-a588-e43246737ed9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|   Category|TotalRevenue|\n",
            "+-----------+------------+\n",
            "|Electronics|      132500|\n",
            "|   Footwear|        8000|\n",
            "|Accessories|        6000|\n",
            "|    Apparel|        3900|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Region\") \\\n",
        "  .count() \\\n",
        "  .orderBy(col(\"count\").desc()) \\\n",
        "  .show(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEyZXfJ388yb",
        "outputId": "c9a55cfc-8740-4795-86a2-54d5936922d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|Region|count|\n",
            "+------+-----+\n",
            "| North|    3|\n",
            "+------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"TotalPrice\") > 30000).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DQ3G_YK9GL8",
        "outputId": "52318518-03b0-400d-ded8-152a8673f483"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----------+--------+---------+------+----------+\n",
            "|OrderID|Product|   Category|Quantity|UnitPrice|Region|TotalPrice|\n",
            "+-------+-------+-----------+--------+---------+------+----------+\n",
            "|   1002| Laptop|Electronics|       1|    55000| South|     55000|\n",
            "|   1005|     TV|Electronics|       1|    40000|  West|     40000|\n",
            "+-------+-------+-----------+--------+---------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Data Transformations\n",
        "\n",
        "1. Create a new column HighValueOrder which is \"Yes\" if TotalPrice > 20,000,\n",
        "else \"No\" .\n",
        "2. Filter and display all high-value orders in the North region.\n",
        "3. Count how many high-value orders exist per region."
      ],
      "metadata": {
        "id": "2r5Z17qx9Ob8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "df = df.withColumn(\"HighValueOrder\", when(col(\"TotalPrice\") > 20000, \"Yes\").otherwise(\"No\"))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pijj7cdU9RcJ",
        "outputId": "58035c65-9c64-4643-9e33-ee85c48d8c69"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+--------+---------+------+----------+--------------+\n",
            "|OrderID|   Product|   Category|Quantity|UnitPrice|Region|TotalPrice|HighValueOrder|\n",
            "+-------+----------+-----------+--------+---------+------+----------+--------------+\n",
            "|   1001|    Mobile|Electronics|       2|    15000| North|     30000|           Yes|\n",
            "|   1002|    Laptop|Electronics|       1|    55000| South|     55000|           Yes|\n",
            "|   1003|   T-Shirt|    Apparel|       3|      500|  East|      1500|            No|\n",
            "|   1004|     Jeans|    Apparel|       2|     1200| North|      2400|            No|\n",
            "|   1005|        TV|Electronics|       1|    40000|  West|     40000|           Yes|\n",
            "|   1006|     Shoes|   Footwear|       4|     2000| South|      8000|            No|\n",
            "|   1007|     Watch|Accessories|       2|     3000|  East|      6000|            No|\n",
            "|   1008|Headphones|Electronics|       3|     2500| North|      7500|            No|\n",
            "+-------+----------+-----------+--------+---------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter((col(\"HighValueOrder\") == \"Yes\") & (col(\"Region\") == \"North\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic5grXc59U03",
        "outputId": "1d1274a8-6135-45e3-8e22-8b1124a9cade"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----------+--------+---------+------+----------+--------------+\n",
            "|OrderID|Product|   Category|Quantity|UnitPrice|Region|TotalPrice|HighValueOrder|\n",
            "+-------+-------+-----------+--------+---------+------+----------+--------------+\n",
            "|   1001| Mobile|Electronics|       2|    15000| North|     30000|           Yes|\n",
            "+-------+-------+-----------+--------+---------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"HighValueOrder\") == \"Yes\") \\\n",
        "  .groupBy(\"Region\") \\\n",
        "  .count() \\\n",
        "  .withColumnRenamed(\"count\", \"HighValueOrderCount\") \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8EELK059Ye-",
        "outputId": "1e11384f-8595-4bd0-b209-014776056904"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+\n",
            "|Region|HighValueOrderCount|\n",
            "+------+-------------------+\n",
            "| South|                  1|\n",
            "|  West|                  1|\n",
            "| North|                  1|\n",
            "+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Save Results\n",
        "\n",
        "Save the transformed DataFrame as a CSV file named high_value_orders.csv with\n",
        "headers."
      ],
      "metadata": {
        "id": "xcoImiIY9cOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"HighValueOrder\") == \"Yes\") \\\n",
        "  .coalesce(1) \\\n",
        "  .write.csv(\"high_value_orders.csv\", header=True, mode=\"overwrite\")\n"
      ],
      "metadata": {
        "id": "vd_mAOVx9f_Y"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}