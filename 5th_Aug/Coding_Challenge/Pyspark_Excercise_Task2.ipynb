{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1: Setup & SparkSession Initialization\n",
        " Tasks:\n",
        "\n",
        " Install and configure PySpark in your local system or Colab.\n",
        "\n",
        " Initialize Spark with:\n",
        "\n",
        " Create a DataFrame from:\n",
        "\n",
        " Show schema, explain data types, and convert to RDD.\n",
        "\n",
        " Print\n",
        ".collect() and\n",
        "df.rdd.map() output."
      ],
      "metadata": {
        "id": "F7F-avHtAuIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XT4c_sJTAc5I"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BotCampus PySpark Practice\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Anjali\", \"Bangalore\", 24),\n",
        "    (\"Ravi\", \"Hyderabad\", 28),\n",
        "    (\"Kavya\", \"Delhi\", 22),\n",
        "    (\"Meena\", \"Chennai\", 25),\n",
        "    (\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "_ElyAv0GA1vX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show schema and data types\n",
        "df.printSchema()\n",
        "\n",
        "# Convert to RDD\n",
        "rdd = df.rdd\n",
        "print(\"\\nRDD:\", rdd.collect())\n",
        "\n",
        "# Example transformation on RDD\n",
        "print(\"\\nName column from RDD:\", rdd.map(lambda row: row.name).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrSJfVuVBJa7",
        "outputId": "9cd40074-1b3d-422a-a4e1-9fd9a53d37b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n",
            "\n",
            "RDD: [Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n",
            "\n",
            "Name column from RDD: ['Anjali', 'Ravi', 'Kavya', 'Meena', 'Arjun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Module 2: RDDs & Transformations"
      ],
      "metadata": {
        "id": "1fbd6vTTBOWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tasks:\n",
        "\n",
        " Split each line into words ( flatMap ).\n",
        "\n",
        " Remove stop words (from , the , etc.).\n",
        "\n",
        " Count each word frequency using reduceByKey\n",
        "\n",
        "Find top 3 most frequent non-stop words.\n"
      ],
      "metadata": {
        "id": "yx-LILh9BQRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "    \"Ravi from Bangalore loved the delivery\",\n",
        "    \"Meena from Hyderabad had a late order\",\n",
        "    \"Ajay from Pune liked the service\",\n",
        "    \"Anjali from Delhi faced UI issues\",\n",
        "    \"Rohit from Mumbai gave positive feedback\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "QeTCUx9zBei2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words\n",
        "stop_words = {\"from\", \"the\", \"and\", \"a\", \"to\", \"of\", \"in\", \"had\"}\n",
        "\n",
        "# remove stopwords\n",
        "words_rdd = feedback.flatMap(lambda line: line.lower().split()) \\\n",
        "    .filter(lambda w: w not in stop_words)\n"
      ],
      "metadata": {
        "id": "AOfIckNHBnbs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word frequency\n",
        "word_counts = words_rdd.map(lambda w: (w, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)"
      ],
      "metadata": {
        "id": "rtz2TqmkBwyf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 3 most frequent\n",
        "top3 = word_counts.takeOrdered(3, key=lambda x: -x[1])\n",
        "\n",
        "print(\"Top 3 frequent words:\", top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEsmxNmvByw8",
        "outputId": "5ddc36d4-e303-490d-dc8e-fa442386e88e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 frequent words: [('loved', 1), ('liked', 1), ('service', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3: DataFrames & Transformation (With Joins)"
      ],
      "metadata": {
        "id": "ZRoZHMeoCadm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Students\n",
        "students = [\n",
        "    (\"Amit\", \"10-A\", 89),\n",
        "    (\"Kavya\", \"10-B\", 92),\n",
        "    (\"Anjali\", \"10-A\", 78),\n",
        "    (\"Rohit\", \"10-B\", 85),\n",
        "    (\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns1 = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "# Attendance\n",
        "attendance = [\n",
        "    (\"Amit\", 24),\n",
        "    (\"Kavya\", 22),\n",
        "    (\"Anjali\", 20),\n",
        "    (\"Rohit\", 25),\n",
        "    (\"Sneha\", 19)\n",
        "]\n",
        "columns2 = [\"name\", \"days_present\"]\n",
        "\n",
        "df_students = spark.createDataFrame(students, columns1)\n",
        "df_attendance = spark.createDataFrame(attendance, columns2)"
      ],
      "metadata": {
        "id": "eaISyXqPCcMN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        " Join both DataFrames on\n",
        "name .\n",
        "\n",
        " Create a new column:\n",
        "attendance_rate = days_present / 25 .\n",
        "\n",
        " Grade students using\n",
        "when :\n",
        " A: >90, B: 80–90, C: <80.\n",
        " Filter students with good grades but poor attendance (<80%)."
      ],
      "metadata": {
        "id": "dkLp8fmiClTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join on name\n",
        "df_joined = df_students.join(df_attendance, on=\"name\", how=\"inner\")\n",
        "\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxV_1utCoK2",
        "outputId": "968601b2-43a0-42ee-c8cb-575b2433069e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+\n",
            "|  name|section|marks|days_present|\n",
            "+------+-------+-----+------------+\n",
            "|  Amit|   10-A|   89|          24|\n",
            "|Anjali|   10-A|   78|          20|\n",
            "| Kavya|   10-B|   92|          22|\n",
            "| Rohit|   10-B|   85|          25|\n",
            "| Sneha|   10-C|   80|          19|\n",
            "+------+-------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attendance rate\n",
        "df_joined = df_joined.withColumn(\"attendance_rate\", col(\"days_present\") / 25)\n",
        "\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTBqvZkeCx67",
        "outputId": "f416eaee-f443-4479-97d7-66ec1c70d3e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+\n",
            "|  name|section|marks|days_present|attendance_rate|\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  Amit|   10-A|   89|          24|           0.96|\n",
            "|Anjali|   10-A|   78|          20|            0.8|\n",
            "| Kavya|   10-B|   92|          22|           0.88|\n",
            "| Rohit|   10-B|   85|          25|            1.0|\n",
            "| Sneha|   10-C|   80|          19|           0.76|\n",
            "+------+-------+-----+------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df_joined.withColumn(\n",
        "    \"grade\",\n",
        "    when(col(\"marks\") > 90, \"A\")\n",
        "    .when((col(\"marks\") >= 80) & (col(\"marks\") <= 90), \"B\")\n",
        "    .otherwise(\"C\")\n",
        ")\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm6jOUP2DBTE",
        "outputId": "5ea10d5d-0c06-49d1-e810-6a00e783eca2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  name|section|marks|days_present|attendance_rate|grade|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  Amit|   10-A|   89|          24|           0.96|    B|\n",
            "|Anjali|   10-A|   78|          20|            0.8|    C|\n",
            "| Kavya|   10-B|   92|          22|           0.88|    A|\n",
            "| Rohit|   10-B|   85|          25|            1.0|    B|\n",
            "| Sneha|   10-C|   80|          19|           0.76|    B|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Ingest CSV & JSON, Save to Parquet"
      ],
      "metadata": {
        "id": "82i0oDS2KEt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# Create sample CSV\n",
        "csv_data = \"\"\"emp_id,name,dept,city,salary\n",
        "101,Anil,IT,Bangalore,80000\n",
        "102,Kiran,HR,Mumbai,65000\n",
        "103,Deepa,Finance,Chennai,72000\n",
        "\"\"\"\n",
        "with open(\"/content/employees.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "# Create sample JSON\n",
        "import json\n",
        "json_data = {\n",
        "    \"id\": 201,\n",
        "    \"name\": \"Nandini\",\n",
        "    \"contact\": {\"email\": \"nandi@example.com\", \"city\": \"Hyderabad\"},\n",
        "    \"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        "}\n",
        "with open(\"/content/employee.json\", \"w\") as f:\n",
        "    json.dump(json_data, f)"
      ],
      "metadata": {
        "id": "GFWw8eb4KJ3T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        " Read both formats into DataFrames.\n",
        "\n",
        "Flatten nested JSON using\n",
        "select ,\n",
        "col ,\n",
        "alias ,\n",
        "explode .\n",
        "\n",
        " Save both as Parquet files partitioned by city."
      ],
      "metadata": {
        "id": "eMOq3qjoKhZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV\n",
        "df_csv = spark.read.csv(\"/content/employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Read JSON\n",
        "df_json = spark.read.json(\"/content/employee.json\", multiLine=True)"
      ],
      "metadata": {
        "id": "6aXY9DXCKjrF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_flat = df_json.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    col(\"contact.email\").alias(\"email\"),\n",
        "    col(\"contact.city\").alias(\"city\"),\n",
        "    explode(\"skills\").alias(\"skill\")\n",
        ")"
      ],
      "metadata": {
        "id": "bgCXi4-RKp8W"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/content/output/employees_csv\")\n",
        "df_flat.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/content/output/employees_json\")"
      ],
      "metadata": {
        "id": "uXYFTRs-Krg4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 5: Spark SQL with Temp Views\n",
        " Tasks:\n",
        "\n",
        " Register the\n",
        "students DataFrame as\n",
        "students_view .\n",
        "\n",
        " Write and run the following queries:\n",
        " -- a) Average marks per section\n",
        "\n",
        " -- b) Top scorer in each section\n",
        "\n",
        " -- c) Count of students in each grade category\n",
        "\n",
        " -- d) Students with marks above class average\n",
        "\n",
        " -- e) Attendance-adjusted performance"
      ],
      "metadata": {
        "id": "5LkG3oCJK0B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register students DataFrame as SQL view\n",
        "df_students.createOrReplaceTempView(\"students_view\")\n",
        "\n",
        "# a) Average marks per section\n",
        "spark.sql(\"\"\"\n",
        "SELECT section, AVG(marks) AS avg_marks\n",
        "FROM students_view\n",
        "GROUP BY section\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAGJ0fZK90X",
        "outputId": "ad2b68f3-9786-4b6f-e482-22146aff1f4e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|section|avg_marks|\n",
            "+-------+---------+\n",
            "|   10-A|     83.5|\n",
            "|   10-B|     88.5|\n",
            "|   10-C|     80.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Top scorer in each section\n",
        "spark.sql(\"\"\"\n",
        "SELECT section, name, marks\n",
        "FROM (\n",
        "    SELECT section, name, marks,\n",
        "           RANK() OVER (PARTITION BY section ORDER BY marks DESC) as rank\n",
        "    FROM students_view\n",
        ") WHERE rank = 1\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsM2Q6-XLGh1",
        "outputId": "25292def-0ed1-4856-dac8-33102dbfa61d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|section| name|marks|\n",
            "+-------+-----+-----+\n",
            "|   10-A| Amit|   89|\n",
            "|   10-B|Kavya|   92|\n",
            "|   10-C|Sneha|   80|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Count of students in each grade category\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    CASE\n",
        "        WHEN marks > 90 THEN 'A'\n",
        "        WHEN marks BETWEEN 80 AND 90 THEN 'B'\n",
        "        ELSE 'C'\n",
        "    END AS grade,\n",
        "    COUNT(*) AS student_count\n",
        "FROM students_view\n",
        "GROUP BY grade\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJyhyxTeLIAe",
        "outputId": "e804e2f8-dded-4599-f130-ca0ad4f7adc4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|grade|student_count|\n",
            "+-----+-------------+\n",
            "|    B|            3|\n",
            "|    A|            1|\n",
            "|    C|            1|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Students with marks above class average\n",
        "spark.sql(\"\"\"\n",
        "SELECT name, section, marks\n",
        "FROM students_view\n",
        "WHERE marks > (SELECT AVG(marks) FROM students_view)\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qso1BvSgLLkE",
        "outputId": "4b972d28-682a-4848-e341-c845ffd85314"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+\n",
            "| name|section|marks|\n",
            "+-----+-------+-----+\n",
            "| Amit|   10-A|   89|\n",
            "|Kavya|   10-B|   92|\n",
            "|Rohit|   10-B|   85|\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Module 6: Partitioned Data & Incremental Loading"
      ],
      "metadata": {
        "id": "hOIejf1uLSD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Load\n",
        "df_students.write.mode(\"overwrite\").partitionBy(\"section\").parquet(\"/content/output/students/\")\n",
        "\n",
        "# Incremental Load\n",
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"/content/output/students/\")\n"
      ],
      "metadata": {
        "id": "y_9KAphPLThd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tasks:\n",
        " List files in\n",
        "output/students/ using Python.\n",
        "\n",
        " Read only partition\n",
        "10-A and list students.\n",
        "\n",
        " Compare before/after counts for section\n",
        "10-A."
      ],
      "metadata": {
        "id": "tjnukCb6LYhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files in output/students/:\", os.listdir(\"/content/output/students/\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6DWbPvyLc4J",
        "outputId": "291b17ca-887e-4276-ea18-475d779386c1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in output/students/: ['._SUCCESS.crc', 'section=10-A', 'section=10-B', '_SUCCESS', 'section=10-C']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read only partition 10-A\n",
        "df_10A = spark.read.parquet(\"/content/output/students/section=10-A\")\n",
        "df_10A.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J0W3toYLlVl",
        "outputId": "f5cb3d55-e2b2-4631-977c-972588d5d465"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  name|marks|\n",
            "+------+-----+\n",
            "|Anjali|   78|\n",
            "| Tejas|   91|\n",
            "|  Amit|   89|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Module 7: ETL Pipeline – End to End\n",
        "  Tasks:\n",
        " Load CSV with inferred schema.\n",
        "\n",
        " Fill null bonuses with\n",
        "2000 .\n",
        "\n",
        " Create\n",
        "total_ctc = salary + bonus .\n",
        "\n",
        " Filter employees with\n",
        "total_ctc > 65000 .\n",
        "\n",
        " Save result in:\n",
        "JSON format.\n",
        "\n",
        " Parquet format partitioned by department"
      ],
      "metadata": {
        "id": "6BN6HR_sLsZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create raw CSV\n",
        "raw_csv = \"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,75000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,68000,4000\n",
        "4,Ramesh,Sales,58000,\n",
        "\"\"\"\n",
        "with open(\"/content/emp_raw.csv\", \"w\") as f:\n",
        "    f.write(raw_csv)\n",
        "\n",
        "# Load CSV\n",
        "df_emp = spark.read.csv(\"/content/emp_raw.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "maHGG-FjLrhX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp = df_emp.fillna({\"bonus\": 2000})\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3oQK6o-L4qS",
        "outputId": "d49ee6ab-eb1a-4314-d6a5-e4b58a8a1e63"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+\n",
            "|emp_id|  name|   dept|salary|bonus|\n",
            "+------+------+-------+------+-----+\n",
            "|     1| Arjun|     IT| 75000| 5000|\n",
            "|     2| Kavya|     HR| 62000| 2000|\n",
            "|     3| Sneha|Finance| 68000| 4000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|\n",
            "+------+------+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp = df_emp.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus\"))\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htEIYMdiL-Ru",
        "outputId": "48ee3314-95d5-46e9-b015-501d9e0a3d75"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+-----+---------+\n",
            "|emp_id|  name|   dept|salary|bonus|total_ctc|\n",
            "+------+------+-------+------+-----+---------+\n",
            "|     1| Arjun|     IT| 75000| 5000|    80000|\n",
            "|     2| Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3| Sneha|Finance| 68000| 4000|    72000|\n",
            "|     4|Ramesh|  Sales| 58000| 2000|    60000|\n",
            "+------+------+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_emp.filter(col(\"total_ctc\") > 65000)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up2TbI80ME7S",
        "outputId": "933691fc-8a06-4c2f-b43a-831646ebeb5c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 75000| 5000|    80000|\n",
            "|     3|Sneha|Finance| 68000| 4000|    72000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.write.mode(\"overwrite\").json(\"/content/output/emp_json\")"
      ],
      "metadata": {
        "id": "hWHaDxTTMMKa"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save in Parquet partitioned by department\n",
        "df_filtered.write.mode(\"overwrite\").partitionBy(\"dept\").parquet(\"/content/output/emp_parquet\")"
      ],
      "metadata": {
        "id": "NxVsaIa5MSa8"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}