{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P7W_3u2quFGh"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark_Excercise1\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Ananya\", \"Bangalore\", 24),\n",
        "    (\"Ravi\", \"Hyderabad\", 28),\n",
        "    (\"Kavya\", \"Delhi\", 22),\n",
        "    (\"Meena\", \"Chennai\", 25)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAgTO-8KuiPC",
        "outputId": "a92d337a-e1b8-448f-be1c-d68ee2b18bf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDDs & Transformations"
      ],
      "metadata": {
        "id": "25bqdDmBuryU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "    \"Ravi from Bangalore loved the mobile app\",\n",
        "    \"Meena from Delhi reported poor response time\",\n",
        "    \"Ajay from Pune liked the delivery speed\",\n",
        "    \"Ananya from Hyderabad had an issue with UI\",\n",
        "    \"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "08NijZ6Xut2c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tasks:\n",
        "\n",
        " Count total number of words.\n",
        "\n",
        " Find top 3 most common words.\n",
        "\n",
        " Remove stop words (from, with, the,etc.).\n",
        "\n",
        " Create a dictionary of word → count."
      ],
      "metadata": {
        "id": "WSDnaLuou7mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Count total words\n",
        "word_count = feedback.flatMap(lambda x: x.split(\" \")).count()\n",
        "print(\"Total Words:\", word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO8Dt4KjvJVs",
        "outputId": "abac22e6-8afd-4774-c2db-05bb061ee59b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Find top 3 most common words\n",
        "from collections import Counter\n",
        "\n",
        "# Flatten and collect words into a Python list\n",
        "words_list = feedback.flatMap(lambda x: x.lower().split()).collect()\n",
        "\n",
        "# Count top 3\n",
        "top3 = Counter(words_list).most_common(3)\n",
        "print(\"Top 3 Words:\", top3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYuLO_A_v-2N",
        "outputId": "edfeaaef-4242-46c0-f7f3-84e7c0c4c3ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Words: [('from', 5), ('the', 2), ('ravi', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 Remove stop words\n",
        "stop_words = {\"the\", \"and\", \"with\", \"from\", \"an\", \"had\", \"was\", \"is\", \"to\", \"a\", \"of\", \"in\"}\n",
        "filtered_words = feedback.flatMap(lambda x: [w for w in x.lower().split() if w not in stop_words])\n",
        "print(\"Filtered Words:\", filtered_words.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L0tn9LgwDlH",
        "outputId": "a7723b03-def7-4b29-e010-df8d8118926f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['ravi', 'bangalore', 'loved', 'mobile', 'app', 'meena', 'delhi', 'reported', 'poor', 'response', 'time', 'ajay', 'pune', 'liked', 'delivery', 'speed', 'ananya', 'hyderabad', 'issue', 'ui', 'rohit', 'mumbai', 'gave', 'positive', 'feedback']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.4 Dictionary of word → count\n",
        "word_dict = dict(Counter(filtered_words.collect()))\n",
        "print(\"Word Dictionary:\", word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sbp1D9DcwGt3",
        "outputId": "92462a82-c4a2-4021-83b2-db5559834369"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Dictionary: {'ravi': 1, 'bangalore': 1, 'loved': 1, 'mobile': 1, 'app': 1, 'meena': 1, 'delhi': 1, 'reported': 1, 'poor': 1, 'response': 1, 'time': 1, 'ajay': 1, 'pune': 1, 'liked': 1, 'delivery': 1, 'speed': 1, 'ananya': 1, 'hyderabad': 1, 'issue': 1, 'ui': 1, 'rohit': 1, 'mumbai': 1, 'gave': 1, 'positive': 1, 'feedback': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrames – Transformations"
      ],
      "metadata": {
        "id": "aUyYTENlwQeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, upper\n",
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "scores = [\n",
        "    (\"Ravi\", \"Math\", 88),\n",
        "    (\"Ananya\", \"Science\", 92),\n",
        "    (\"Kavya\", \"English\", 79),\n",
        "    (\"Ravi\", \"English\", 67),\n",
        "    (\"Neha\", \"Math\", 94),\n",
        "    (\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)"
      ],
      "metadata": {
        "id": "w08aY4DYwSwc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        " Add grade column (>=90 → A, 80-89 → B,\n",
        "70-79 → C, else D).\n",
        "\n",
        " Group by subject, find average score.\n",
        "\n",
        " Use when and otherwise to classify subject difficulty (Difficult).\n",
        "\n",
        " Rank students per subject using Window function.\n",
        "\n",
        " Apply UDF to format names (e.g., make all uppercase)."
      ],
      "metadata": {
        "id": "rU15Wrvxwote"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Add Grade Column\n",
        "df_scores = df_scores.withColumn(\n",
        "    \"grade\",\n",
        "    when(col(\"score\") >= 90, \"A\")\n",
        "    .when((col(\"score\") >= 80) & (col(\"score\") <= 89), \"B\")\n",
        "    .when((col(\"score\") >= 70) & (col(\"score\") <= 79), \"C\")\n",
        "    .otherwise(\"D\")\n",
        ")"
      ],
      "metadata": {
        "id": "VrXoiK1fw1F6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Group by subject - average score\n",
        "df_scores.groupBy(\"subject\").agg(F.avg(\"score\").alias(\"avg_score\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBNp7rDew7Ij",
        "outputId": "efa72c30-9b48-4b9f-cb58-909cf7bfe942"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "|English|     73.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 Classify difficulty\n",
        "df_scores = df_scores.withColumn(\n",
        "    \"difficulty\",\n",
        "    when(col(\"subject\").isin(\"Math\", \"Science\"), \"Difficult\").otherwise(\"Easy\")\n",
        ")"
      ],
      "metadata": {
        "id": "9g-X49lHw_qg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4 Rank students per subject\n",
        "windowSpec = Window.partitionBy(\"subject\").orderBy(col(\"score\").desc())\n",
        "df_scores = df_scores.withColumn(\"rank\", F.rank().over(windowSpec))"
      ],
      "metadata": {
        "id": "dCjJ0we4xLjm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.5 Apply UDF to format names\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "upper_udf = udf(lambda x: x.upper(), StringType())\n",
        "df_scores = df_scores.withColumn(\"name_upper\", upper_udf(col(\"name\")))\n",
        "\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09z0-QtFxVXU",
        "outputId": "d8fa03fe-c5c3-4ad3-92e4-de3ba8bbc564"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+----------+\n",
            "|  name|subject|score|grade|difficulty|rank|name_upper|\n",
            "+------+-------+-----+-----+----------+----+----------+\n",
            "| Kavya|English|   79|    C|      Easy|   1|     KAVYA|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|      RAVI|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|      NEHA|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|      RAVI|\n",
            "|Ananya|Science|   92|    A| Difficult|   1|    ANANYA|\n",
            "| Meena|Science|   85|    B| Difficult|   2|     MEENA|\n",
            "+------+-------+-----+-----+----------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingest CSV & JSON – Save to Parquet"
      ],
      "metadata": {
        "id": "klbjAZlhxfD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "csv_data = \"\"\"id,name,department,city,salary\n",
        "1,Amit,IT,Bangalore,78000\n",
        "2,Kavya,HR,Chennai,62000\n",
        "3,Arjun,Finance,Hyderabad,55000\n",
        "\"\"\"\n",
        "with open(\"/content/students.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n"
      ],
      "metadata": {
        "id": "lgy8IZcZxhQq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = [\n",
        "    {\n",
        "        \"id\": 101,\n",
        "        \"name\": \"Sneha\",\n",
        "        \"address\": {\"city\": \"Mumbai\", \"pincode\": 400001},\n",
        "        \"skills\": [\"Python\", \"Spark\"]\n",
        "    }\n",
        "]\n",
        "import json\n",
        "with open(\"/content/employee_nested.json\", \"w\") as f:\n",
        "    json.dump(json_data, f)"
      ],
      "metadata": {
        "id": "SjrLPqd4xuDD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tasks:\n",
        "\n",
        " Load both datasets into PySpark.\n",
        "\n",
        " Print schema and infer nested structure.\n",
        "\n",
        " Flatten the JSON (use explode, select ,\n",
        "alias ).\n",
        "\n",
        " Convert both to Parquet and write to\n",
        "/tmp/output ."
      ],
      "metadata": {
        "id": "bWHKWJCZxxV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV\n",
        "df_csv = spark.read.csv(\"/content/students.csv\", header=True, inferSchema=True)\n",
        "df_csv.show()\n",
        "\n",
        "# Load JSON\n",
        "df_json = spark.read.json(\"/content/employee_nested.json\", multiLine=True)\n",
        "df_json.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqmGsQRyxvjb",
        "outputId": "296f2f19-5193-418c-f1b6-23d2f9ddff7f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n",
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_flat = df_json.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    col(\"address.city\").alias(\"city\"),\n",
        "    col(\"address.pincode\").alias(\"pincode\"),\n",
        "    F.explode(\"skills\").alias(\"skill\")\n",
        ")\n",
        "df_flat.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP0Qj2Nwx-nr",
        "outputId": "0262a8eb-30c8-4cfe-bb3e-6f6f60225917"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-------+------+\n",
            "| id| name|  city|pincode| skill|\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai| 400001|Python|\n",
            "|101|Sneha|Mumbai| 400001| Spark|\n",
            "+---+-----+------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.write.mode(\"overwrite\").parquet(\"/tmp/output/students\")\n",
        "df_flat.write.mode(\"overwrite\").parquet(\"/tmp/output/employees\")"
      ],
      "metadata": {
        "id": "z2LiPQFTyWFO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark SQL – Temp Views & Queries"
      ],
      "metadata": {
        "id": "3rMvIxt_yeO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Exercise 5.1 Create view from exam scores and run:\n",
        "\n",
        " -- a) Top scorer per subject\n",
        "\n",
        " --b) Count of students per grade\n",
        "\n",
        " -- c) Students with multiple subjects\n",
        "\n",
        " -- d) Subjects with average score above 85"
      ],
      "metadata": {
        "id": "iVXORtrPymIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.createOrReplaceTempView(\"exam_scores\")\n"
      ],
      "metadata": {
        "id": "QCjvzJ5nyhAh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT subject, name, MAX(score) AS top_score\n",
        "FROM exam_scores\n",
        "GROUP BY subject, name\n",
        "ORDER BY subject, top_score DESC\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdYh9V033Ib_",
        "outputId": "4c7a61a5-3458-42cd-eb35-338edb1dff1d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---------+\n",
            "|subject|  name|top_score|\n",
            "+-------+------+---------+\n",
            "|English| Kavya|       79|\n",
            "|English|  Ravi|       67|\n",
            "|   Math|  Neha|       94|\n",
            "|   Math|  Ravi|       88|\n",
            "|Science|Ananya|       92|\n",
            "|Science| Meena|       85|\n",
            "+-------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT grade, COUNT(*) AS student_count\n",
        "FROM exam_scores\n",
        "GROUP BY grade\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdUpwTED3w_f",
        "outputId": "246b0693-332f-431e-d867-f6d47333b045"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|grade|student_count|\n",
            "+-----+-------------+\n",
            "|    B|            2|\n",
            "|    C|            1|\n",
            "|    A|            2|\n",
            "|    D|            1|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT name, COUNT(subject) AS subject_count\n",
        "FROM exam_scores\n",
        "GROUP BY name\n",
        "HAVING subject_count > 1\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47qmpZz73yuZ",
        "outputId": "dcb59cb5-4613-4850-a190-cf135b5e6537"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|name|subject_count|\n",
            "+----+-------------+\n",
            "|Ravi|            2|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT subject, AVG(score) AS avg_score\n",
        "FROM exam_scores\n",
        "GROUP BY subject\n",
        "HAVING avg_score > 85\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxW_04jk31n-",
        "outputId": "9bd69e2b-ee7f-44d2-fb11-50354a19cd64"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Exercise 5.2 Create another DataFrame\n",
        "attendance(name, days_present) and:\n",
        "\n",
        "Join with scores\n",
        "\n",
        " Calculate attendance-adjusted grade:\n",
        " If days_present < 20 → downgrade grade by one level"
      ],
      "metadata": {
        "id": "UNXvE3PW38wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New DataFrame for attendance\n",
        "attendance_data = [(\"Ravi\", 22), (\"Ananya\", 18), (\"Kavya\", 25), (\"Neha\", 21), (\"Meena\", 19)]\n",
        "columns = [\"name\", \"days_present\"]\n",
        "df_attendance = spark.createDataFrame(attendance_data, columns)"
      ],
      "metadata": {
        "id": "9amoM5FX4AZz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df_scores.join(df_attendance, on=\"name\", how=\"inner\")\n",
        "\n",
        "df_joined.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEZLx7E94Hdh",
        "outputId": "9079e261-c047-4707-ed25-e02e5d75bbf0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+----------+------------+\n",
            "|  name|subject|score|grade|difficulty|rank|name_upper|days_present|\n",
            "+------+-------+-----+-----+----------+----+----------+------------+\n",
            "|Ananya|Science|   92|    A| Difficult|   1|    ANANYA|          18|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|      RAVI|          22|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|      RAVI|          22|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|      NEHA|          21|\n",
            "| Meena|Science|   85|    B| Difficult|   2|     MEENA|          19|\n",
            "| Kavya|English|   79|    C|      Easy|   1|     KAVYA|          25|\n",
            "+------+-------+-----+-----+----------+----+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "grade_map = {\n",
        "    \"A\": \"B\",\n",
        "    \"B\": \"C\",\n",
        "    \"C\": \"D\",\n",
        "    \"D\": \"D\"\n",
        "}\n",
        "\n",
        "def downgrade(grade):\n",
        "    return grade_map.get(grade, grade)\n",
        "\n",
        "downgrade_udf = udf(lambda g: downgrade(g), StringType())\n",
        "\n",
        "df_adjusted = df_joined.withColumn(\n",
        "    \"adjusted_grade\",\n",
        "    when(col(\"days_present\") < 20, downgrade_udf(col(\"grade\"))).otherwise(col(\"grade\"))\n",
        ")\n",
        "\n",
        "df_adjusted.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F18aBFEr4M8N",
        "outputId": "97959b7c-7d9f-47c2-e72a-efbadff615b0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+----------+------------+--------------+\n",
            "|  name|subject|score|grade|difficulty|rank|name_upper|days_present|adjusted_grade|\n",
            "+------+-------+-----+-----+----------+----+----------+------------+--------------+\n",
            "|Ananya|Science|   92|    A| Difficult|   1|    ANANYA|          18|             B|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|      RAVI|          22|             B|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|      RAVI|          22|             D|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|      NEHA|          21|             A|\n",
            "| Meena|Science|   85|    B| Difficult|   2|     MEENA|          19|             C|\n",
            "| Kavya|English|   79|    C|      Easy|   1|     KAVYA|          25|             C|\n",
            "+------+-------+-----+-----+----------+----+----------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Partitioned Load (Full + Incremental)"
      ],
      "metadata": {
        "id": "H_wGp0yi4_JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.write.partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "b8VkB6Je4VXr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_inc = spark.createDataFrame(incremental, columns)\n",
        "\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")\n"
      ],
      "metadata": {
        "id": "13MznxD65Pm7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task:\n",
        "\n",
        " List all folders inside\n",
        "\n",
        "Read only\n",
        "/tmp/scores/\n",
        " Math partition and display all entries."
      ],
      "metadata": {
        "id": "KxGM1rVe5qPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read only 'Math' partition\n",
        "df_math = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "df_math.show()\n",
        "\n",
        "# List folders/files (for Colab only)\n",
        "import os\n",
        "print(\"Folders under /tmp/scores/:\")\n",
        "print(os.listdir(\"/tmp/scores\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "audiyqJR5gLF",
        "outputId": "4e24b5ca-4266-4d01-fa7f-6ed0114c9381"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+----------+----+----------+\n",
            "| name|score|grade|difficulty|rank|name_upper|\n",
            "+-----+-----+-----+----------+----+----------+\n",
            "| Neha|   94|    A| Difficult|   1|      NEHA|\n",
            "| Ravi|   88|    B| Difficult|   2|      RAVI|\n",
            "|Meena|   93| NULL|      NULL|NULL|      NULL|\n",
            "+-----+-----+-----+----------+----+----------+\n",
            "\n",
            "Folders under /tmp/scores/:\n",
            "['._SUCCESS.crc', 'subject=Science', 'subject=English', '_SUCCESS', 'subject=Math']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL: Clean, Transform, Load"
      ],
      "metadata": {
        "id": "RFPTNe5v7EWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "\n",
        " Load data with header.\n",
        "\n",
        " Fill missing bonus with 2000.\n",
        "\n",
        " Calculate total_ctc = salary + bonus .\n",
        "\n",
        " Filter where total_ctc > 60,000.\n",
        "\n",
        " Save final DataFrame to Parquet and JSON."
      ],
      "metadata": {
        "id": "dJEUadtF7UYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = \"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,78000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,55000,3000\n",
        "\"\"\"\n",
        "with open(\"/content/employees_raw.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "# Load with header\n",
        "df_emp = spark.read.csv(\"/content/employees_raw.csv\", header=True, inferSchema=True)\n",
        "df_emp.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdeYqE7r7Gol",
        "outputId": "d291fbdf-1e29-4d70-f801-0ef0b47214cd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| NULL|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp = df_emp.fillna({\"bonus\": 2000})\n"
      ],
      "metadata": {
        "id": "N2VuBz7c7axi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emp = df_emp.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus\"))\n",
        "df_emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dt48tdD7e2Z",
        "outputId": "19384e86-7ff2-4d37-91fa-33168a1c7a6b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3|Sneha|Finance| 55000| 3000|    58000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_emp.filter(col(\"total_ctc\") > 60000)\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MWz9kpd7szf",
        "outputId": "fc28e968-675f-4519-fcc7-2d06416b9e07"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----+------+-----+---------+\n",
            "|emp_id| name|dept|salary|bonus|total_ctc|\n",
            "+------+-----+----+------+-----+---------+\n",
            "|     1|Arjun|  IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|  HR| 62000| 2000|    64000|\n",
            "+------+-----+----+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.write.mode(\"overwrite\").parquet(\"/tmp/emp_final_parquet\")\n",
        "df_filtered.write.mode(\"overwrite\").json(\"/tmp/emp_final_json\")\n"
      ],
      "metadata": {
        "id": "l8-wD9ch7w3B"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}